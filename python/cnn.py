# -*- coding: utf-8 -*-
"""cnn_colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/183q4XbN8JewhCrKqkg-Y2vTWZh2rzYGN
"""

import tensorflow as tf

import sys
import keras
from keras.models import Sequential
from keras.layers import Dense, Flatten, LSTM, BatchNormalization, InputLayer
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import os
from PIL import Image
import cv2

window_size = 10
uwb_fs = 20
biopac_fs = 500
root_dir = '/content/drive/MyDrive/Colab Notebooks/Data'


def get_img_path(root_dir):
    file_list = []
    for (root, dirs, files) in os.walk(root_dir):
        for file_name in files:
            if (file_name.endswith("_gray.npy")):
                file_list.append(root + "/" + file_name)
    return file_list

def get_peak_path(root_dir):
    file_list = []
    for (root, dirs, files) in os.walk(root_dir):
        for file_name in files:
            if (file_name.endswith("_ref.npy")):
                file_list.append(root + "/" + file_name)
    return file_list

data_path_list = get_img_path(root_dir)
ref_path_list = get_peak_path(root_dir)
data_path_list.sort()
ref_path_list.sort()

if len(data_path_list) != len(ref_path_list):
    print("데이터 크기 불일치")
    sys.exit("오류")


data_list = []
ref_list = []
for data_path in data_path_list:
    tmp = np.load(data_path)
    for i in range(110):
      resized_img = cv2.resize(tmp[i], dsize=(28, 28), interpolation=cv2.INTER_CUBIC)
      data_list.append(resized_img)
    #data_list.append(tmp)
for ref_path in ref_path_list:
    ref_list.append(np.load(ref_path))

data_list = np.array(data_list)
#tmp_list = np.array(data_list)
#data_list = tmp_list.reshape(len(data_list)*110, 31, 200, 3)
data_list = data_list.reshape(len(data_list), 28, 28, 1)

ref_list = np.reshape(np.array(ref_list),(1,len(ref_list)*110))
ref_list = ref_list[0]

data_list = data_list / 255.0



#ref_list = tf.keras.utils.to_categorical(ref_list, num_classes=6)

x_train, x_test, y_train, y_test = train_test_split(data_list, ref_list, test_size=0.1)

img_rows = 32
img_cols = 32

batch_size = 32
epochs = 5

"""model = keras.models.Sequential([
    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape = [img_rows, img_cols,3]),
    keras.layers.MaxPooling2D(),
    keras.layers.Conv2D(64, (2, 2), activation='relu'),
    keras.layers.MaxPooling2D(),
    keras.layers.Conv2D(64, (2, 2), activation='relu'),
    keras.layers.Flatten(),
    keras.layers.Dense(100, activation='relu'),
    keras.layers.Dense(6, activation ='softmax')
])
model.summary()
model.compile(optimizer='adam',
             loss = 'categorical_crossentropy',
             metrics=['accuracy'])
model.fit(x_train, y_train, batch_size = 128, epochs=20, validation_data=(x_test, y_test))

from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout

# build a sequential model
model = Sequential()
model.add(InputLayer(input_shape=(224, 224, 3)))

# 1st conv block
model.add(Conv2D(25, (5, 5), activation='relu', strides=(1, 1), padding='same'))
model.add(MaxPool2D(pool_size=(2, 2), padding='same'))
# 2nd conv block
model.add(Conv2D(50, (5, 5), activation='relu', strides=(2, 2), padding='same'))
model.add(MaxPool2D(pool_size=(2, 2), padding='same'))
model.add(BatchNormalization())
# 3rd conv block
model.add(Conv2D(70, (3, 3), activation='relu', strides=(2, 2), padding='same'))
model.add(MaxPool2D(pool_size=(2, 2), padding='valid'))
model.add(BatchNormalization())
# ANN block
model.add(Flatten())
model.add(Dense(units=100, activation='relu'))
model.add(Dense(units=100, activation='relu'))
model.add(Dropout(0.25))
# output layer
model.add(Dense(units=6, activation='softmax'))
model.summary()
# compile model
model.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])
# fit on data for 30 epochs
model.fit(x_train, y_train, batch_size = 32, epochs=25, validation_data=(x_test, y_test))
"""

from tensorflow.keras import layers, models
model = models.Sequential([
    layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1),
                  padding='same', activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    layers.Conv2D(64, kernel_size=(2, 2), activation='relu', padding='same'),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Dropout(0.25),
    layers.Flatten(),
    layers.Dense(1000, activation='relu'),
    layers.Dense(5, activation='softmax')
])

model.summary()

import time

model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])

start_time = time.time()
#early_stopping = keras.callbacks.EarlyStopping(patience=5)

hist = model.fit(x_train, y_train, batch_size = 64, epochs=13, verbose = 1, validation_data=(x_test, y_test))

print(f'Fit Time :{time.time() - start_time}')

import matplotlib.pyplot as plt

plot_target = ['loss' , 'accuracy', 'val_loss', 'val_accuracy']
plt.figure(figsize=(12, 8))

for each in plot_target:
    plt.plot(hist.history[each], label = each)
plt.legend()
plt.grid()
plt.show()

score = model.evaluate(x_test, y_test)
print(f'Test Loss : {score[0]}')
print(f'Test Accuracy  : {score[1]}')

import random
predicted_result = model.predict(x_test)
predicted_labels = np.argmax(predicted_result,  axis=1)
wrong_result = []
for n in range(0, len(y_test)):
    if predicted_labels[n] != y_test[n]:
        wrong_result.append(n)
samples = random.choices(population=wrong_result, k =16)

plt.figure(figsize=(14, 12))

for idx, n in enumerate(samples):
    plt.subplot(4, 4, idx + 1)
    plt.imshow(x_test[n].reshape(28,28), cmap = 'Greys', interpolation='nearest')
    plt.title('predicted_val ' + str(predicted_labels[n]) + ', ref_val ' + str(y_test[n]))
    plt.axis('off')
    
plt.show()